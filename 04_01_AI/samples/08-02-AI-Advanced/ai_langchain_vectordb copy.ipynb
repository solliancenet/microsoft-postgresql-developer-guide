{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/integrations/vectorstores/pgembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL as Vector Database with LangChain\n",
    "\n",
    "In this lab you will utilize the built in Document Loading capabilities of LangChain to embed documents into PostgreSQL.\n",
    "\n",
    "You will then use a similarity search to search the indexed and embedded documents followed by using a query Retriever to add context to an Open AI request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install psycopg2\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#depening on your langchain version...\n",
    "#from langchain_community.document_loaders import TextLoader\n",
    "#from langchain_community.document_loaders import PdfLoader\n",
    "#from langchain_community.vectorstores import PGEmbedding\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.document_loaders import UnstructuredXMLLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.vectorstores.pgvector import DistanceStrategy\n",
    "\n",
    "username = 'wsuser'\n",
    "password = 'Solliance123'\n",
    "host = 'pgsqldevilkdittaflex16.postgres.database.azure.com'\n",
    "port = '5432'\n",
    "dbname = 'ailabs'\n",
    "\n",
    "connection_string = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{dbname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconn = psycopg2.connect(host=host, user=username, password=password,\n",
    "    port=port, database=dbname , connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "\n",
    "cur = dbconn.cursor()\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "\n",
    "register_vector(dbconn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the openai embeddings\n",
    "embedding_model = \"embeddings\" #this is the name of the model deployment in azure open ai (not the type of model)\n",
    "azure_endpoint = \"YOUR_API_URL\"  #https://fllm4693d-openai.openai.azure.com/\n",
    "azure_key = 'YOUR_API_KEY'  #abcdf0a8efa1432796a3dab8f0e61234\n",
    "\n",
    "azure_endpoint = \"https://fllm4693d-openai.openai.azure.com/\"\n",
    "azure_key = '1b13f0a8efa1432796a3dab8f0e62a2d'\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "                deployment=embedding_model,\n",
    "                openai_api_base=azure_endpoint,\n",
    "                openai_api_key=azure_key,\n",
    "                openai_api_type=\"azure\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "rootLogger = logging.getLogger()\n",
    "\n",
    "logPath = \"./logs\"\n",
    "fileName = \"langchain\"\n",
    "\n",
    "fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(consoleHandler)\n",
    "\n",
    "rootLogger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_document(db, doc, collection_name=None, collection_id=None):\n",
    "\n",
    "    import uuid\n",
    "\n",
    "    #generate the embeddings\n",
    "    doc_embeddings = embeddings.embed_documents(doc.page_content)\n",
    "\n",
    "    #collection = db.get_collection(db._make_session())\n",
    "\n",
    "    collection_id = \"a1a2bb38-562b-4d46-9475-268fe3bc4e4d\"\n",
    "\n",
    "    #insert with manual sql\n",
    "    sql = \"INSERT INTO langchain_pg_embedding (collection_id, embedding, document, cmetadata, custom_id, uuid) VALUES ({0},ARRAY {1}, {2}, {3},{4}, {5})\".format(collection_id, doc_embeddings, doc.page_content, None, None, uuid.uuid4())\n",
    "    dbconn.execute_sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/azure_openai_infographic.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "collection_name = \"open_ai_pdf\"\n",
    "\n",
    "db = PGVector(\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_string,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.from_documents(\n",
    "    documents= docs,\n",
    "    embedding = embeddings,\n",
    "    collection_name= collection_name,\n",
    "    distance_strategy = DistanceStrategy.COSINE,\n",
    "    connection_string=connection_string,\n",
    "    logger=rootLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    insert_document(db, doc, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilize a simple similarity search\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "\n",
    "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the text loader and splitter to break apart the document into chunks\n",
    "loader = TextLoader(\"./data/msft_earnings_call_transcript.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "collection_name = \"msft_earnings\"\n",
    "\n",
    "db = PGVector(\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_string,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "#you can do a full database load with the creation of the collection and the embedding table.\n",
    "#db = db.from_documents(\n",
    "#    documents= docs,\n",
    "#    embedding = embeddings,\n",
    "#    collection_name= collection_name,\n",
    "#    distance_strategy = DistanceStrategy.COSINE,\n",
    "#    connection_string=connection_string)\n",
    "\n",
    "#you can also manually import the documents into a target collection\n",
    "for i in range(0, len(docs), 1):\n",
    "    temp_docs = docs[i:i+1]\n",
    "    db.add_documents( documents=temp_docs, collection_name=collection_name, connection_string=connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilize a query retriver\n",
    "prompt_prefix = \"You are a question and answering system. You are given a question and a context. You must answer the question based on the context provided. {context} Question: {question}\"\n",
    "deployment_name = \"completions\"\n",
    "api_version = \"\"\n",
    "model_version = \"\"\n",
    "\n",
    "question = \"Which analytis were on the call?\"\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#create a simple QA chain\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_prefix,\n",
    "    input_variables=[\"context\", \"question\"], #\"summaries\", \"question\"\n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                        temperature=0,\n",
    "                        openai_api_base=azure_endpoint,\n",
    "                        openai_api_key=azure_key,\n",
    "                        openai_api_type=\"azure\",\n",
    "                        openai_api_version=api_version,\n",
    "                        model_version=model_version)\n",
    "\n",
    "llm_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    #memory=memory,\n",
    "    #callbacks = [self.handler],\n",
    "    chain_type=\"stuff\",\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = llm_chain.invoke(question, return_only_outputs=True)['answer']\n",
    "\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
